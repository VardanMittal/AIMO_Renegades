{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":73231,"databundleVersionId":8365361,"sourceType":"competition"},{"sourceId":8009768,"sourceType":"datasetVersion","datasetId":4717827},{"sourceId":8524712,"sourceType":"datasetVersion","datasetId":4766079},{"sourceId":33547,"sourceType":"modelInstanceVersion","modelInstanceId":28079}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nos.environ[\"KERAS_BACKEND\"] = \"torch\" # you can also use tensorflow or torch\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.9\" # avoid memory fragmentation on JAX backend.\n\nimport keras\nimport keras_nlp\n\nfrom tqdm.notebook import tqdm\ntqdm.pandas() # progress bar for pandas\n\nimport plotly.graph_objs as go\nimport plotly.express as px\nfrom IPython.display import display, Markdown\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-14T06:47:14.009457Z","iopub.execute_input":"2024-06-14T06:47:14.009825Z","iopub.status.idle":"2024-06-14T06:47:33.136899Z","shell.execute_reply.started":"2024-06-14T06:47:14.009774Z","shell.execute_reply":"2024-06-14T06:47:33.135854Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/keras-lib-dataset/keras-3.3.3-py3-none-any.whl\n/kaggle/input/keras-lib-dataset/keras_nlp-0.12.1-py3-none-any.whl\n/kaggle/input/llama-3/transformers/8b-hf/1/model.safetensors.index.json\n/kaggle/input/llama-3/transformers/8b-hf/1/model-00003-of-00004.safetensors\n/kaggle/input/llama-3/transformers/8b-hf/1/config.json\n/kaggle/input/llama-3/transformers/8b-hf/1/LICENSE\n/kaggle/input/llama-3/transformers/8b-hf/1/model-00001-of-00004.safetensors\n/kaggle/input/llama-3/transformers/8b-hf/1/USE_POLICY.md\n/kaggle/input/llama-3/transformers/8b-hf/1/tokenizer.json\n/kaggle/input/llama-3/transformers/8b-hf/1/tokenizer_config.json\n/kaggle/input/llama-3/transformers/8b-hf/1/example_text_completion.py\n/kaggle/input/llama-3/transformers/8b-hf/1/requirements.txt\n/kaggle/input/llama-3/transformers/8b-hf/1/model-00004-of-00004.safetensors\n/kaggle/input/llama-3/transformers/8b-hf/1/eval_details.md\n/kaggle/input/llama-3/transformers/8b-hf/1/special_tokens_map.json\n/kaggle/input/llama-3/transformers/8b-hf/1/model-00002-of-00004.safetensors\n/kaggle/input/llama-3/transformers/8b-hf/1/example_chat_completion.py\n/kaggle/input/llama-3/transformers/8b-hf/1/setup.py\n/kaggle/input/llama-3/transformers/8b-hf/1/generation_config.json\n/kaggle/input/ai-mathematical-olympiad-prize/sample_submission.csv\n/kaggle/input/ai-mathematical-olympiad-prize/AIMO Prize - Note on Language and Notation.pdf\n/kaggle/input/ai-mathematical-olympiad-prize/train.csv\n/kaggle/input/ai-mathematical-olympiad-prize/test.csv\n/kaggle/input/ai-mathematical-olympiad-prize/aimo/competition.cpython-310-x86_64-linux-gnu.so\n/kaggle/input/ai-mathematical-olympiad-prize/aimo/__init__.py\n/kaggle/input/math-qsa-dataset/train.csv\n/kaggle/input/math-qsa-dataset/test.csv\n","output_type":"stream"},{"name":"stderr","text":"2024-06-14 06:47:22.327618: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-14 06:47:22.327720: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-14 06:47:22.468870: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install pylatexenc","metadata":{"execution":{"iopub.status.busy":"2024-06-14T06:47:33.138847Z","iopub.execute_input":"2024-06-14T06:47:33.139984Z","iopub.status.idle":"2024-06-14T06:47:48.742305Z","shell.execute_reply.started":"2024-06-14T06:47:33.139944Z","shell.execute_reply":"2024-06-14T06:47:48.741202Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting pylatexenc\n  Downloading pylatexenc-2.10.tar.gz (162 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: pylatexenc\n  Building wheel for pylatexenc (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pylatexenc: filename=pylatexenc-2.10-py3-none-any.whl size=136816 sha256=96889a4bbaa66fb53638543c7383096a4cbb3cc0ce85cebaf4e1f749eaae8840\n  Stored in directory: /root/.cache/pip/wheels/d3/31/8b/e09b0386afd80cfc556c00408c9aeea5c35c4d484a9c762fd5\nSuccessfully built pylatexenc\nInstalling collected packages: pylatexenc\nSuccessfully installed pylatexenc-2.10\n","output_type":"stream"}]},{"cell_type":"code","source":"class CFG:\n    seed = 42\n    dataset_path = \"/kaggle/input/ai-mathematical-olympiad-prize\"\n    preset = \"gemma_1.1_instruct_2b_en\" # name of pretrained Gemma\n    sequence_length = 512 # max size of input sequence for training\n    batch_size = 1 # size of the input batch in training\n    epochs = 1 # number of epochs to train\nkeras.utils.set_random_seed(CFG.seed)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T06:47:48.743715Z","iopub.execute_input":"2024-06-14T06:47:48.744039Z","iopub.status.idle":"2024-06-14T06:47:48.753064Z","shell.execute_reply.started":"2024-06-14T06:47:48.744011Z","shell.execute_reply":"2024-06-14T06:47:48.752305Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df1 = pd.read_csv(\"/kaggle/input/math-qsa-dataset/train.csv\")\ndf2 = pd.read_csv(\"/kaggle/input/math-qsa-dataset/test.csv\")\ndf = pd.concat([df1, df2], axis=0)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T06:47:48.755430Z","iopub.execute_input":"2024-06-14T06:47:48.755708Z","iopub.status.idle":"2024-06-14T06:47:48.956341Z","shell.execute_reply.started":"2024-06-14T06:47:48.755684Z","shell.execute_reply":"2024-06-14T06:47:48.955450Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def is_integer(text):\n    try:\n        if int(text) >= 0:\n            return True\n        else:\n            return False\n    except ValueError:\n        return False\n    \ndf[\"is_integer\"] = df.answer.map(is_integer)\ndf = df[df.is_integer].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T06:47:48.957633Z","iopub.execute_input":"2024-06-14T06:47:48.958020Z","iopub.status.idle":"2024-06-14T06:47:48.989426Z","shell.execute_reply.started":"2024-06-14T06:47:48.957986Z","shell.execute_reply":"2024-06-14T06:47:48.988594Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-06-14T06:47:48.990705Z","iopub.execute_input":"2024-06-14T06:47:48.991069Z","iopub.status.idle":"2024-06-14T06:47:49.017533Z","shell.execute_reply.started":"2024-06-14T06:47:48.991037Z","shell.execute_reply":"2024-06-14T06:47:49.016596Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 7356 entries, 0 to 7355\nData columns (total 6 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   problem     7356 non-null   object\n 1   level       7356 non-null   object\n 2   type        7356 non-null   object\n 3   solution    7356 non-null   object\n 4   answer      7356 non-null   object\n 5   is_integer  7356 non-null   bool  \ndtypes: bool(1), object(5)\nmemory usage: 294.7+ KB\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Preprocessing Pipeline","metadata":{}},{"cell_type":"code","source":"import re\nimport nltk\nimport pandas as pd\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\n\n# Ensure the necessary NLTK data files are downloaded\nnltk.download('punkt')\nnltk.download('stopwords')\n\nclass Preprocessing:\n    def __init__(self):\n        self.stop_words = set(stopwords.words('english'))\n\n    def convert_draw_command(self, draw_command):\n        pattern_pentagon = re.compile(r'draw\\(\\((.*?)\\)--\\((.*?)\\)--\\((.*?)\\)--\\((.*?)\\)--\\((.*?)\\)--\\((.*?)\\)--cycle.*?\\);')\n        match_pentagon = pattern_pentagon.match(draw_command)\n        if match_pentagon:\n            coords = match_pentagon.groups()\n            return f\"A regular pentagon with vertices at {coords[0]}, {coords[1]}, {coords[2]}, {coords[3]}, and {coords[4]}.\"\n        \n        pattern_hexagon = re.compile(r'draw\\(\\((.*?)\\)--\\((.*?)\\)--\\((.*?)\\)--\\((.*?)\\)--\\((.*?)\\),.*?\\);')\n        match_hexagon = pattern_hexagon.match(draw_command)\n        if match_hexagon:\n            coords = match_hexagon.groups()\n            return f\"A regular hexagon with vertices at {coords[0]}, {coords[1]}, {coords[2]}, {coords[3]}, and {coords[4]}.\"\n        \n        return \"\"\n\n    def convert_dot_label_commands(self, text):\n        pattern_dot = re.compile(r'dot\\(\\((.*?)\\)\\);')\n        text = pattern_dot.sub(r'A point at \\1.', text)\n        \n        pattern_label = re.compile(r'label\\(\"(.*?)\",\\((.*?)\\),.*?\\);')\n        text = pattern_label.sub(r'The point \\1 is at coordinates \\2.', text)\n        \n        return text\n\n    def preprocess_text(self, text):\n        # Remove the [asy] tags\n        text = re.sub(r'\\[asy\\]', '', text)\n        text = re.sub(r'\\[\\/asy\\]', '', text)\n\n        # Split the text into commands\n        commands = text.split('\\n')\n\n        readable_text = []\n        for command in commands:\n            if 'draw' in command:\n                readable_text.append(self.convert_draw_command(command))\n            else:\n                readable_text.append(self.convert_dot_label_commands(command))\n\n        readable_text = ' '.join(readable_text)\n\n        # Tokenize into sentences\n        sentences = sent_tokenize(readable_text)\n\n        # Remove stop words and tokenize the remaining words\n        filtered_sentences = []\n        for sentence in sentences:\n            word_tokens = word_tokenize(sentence)\n            filtered_sentence = [word for word in word_tokens if word.lower() not in self.stop_words]\n            filtered_sentences.append(' '.join(filtered_sentence))\n\n        filtered_text = ' '.join(filtered_sentences)\n        return filtered_text\n\n    def process_dataframe(self, df, text_column):\n        df[f'{text_column}'] = df[text_column].apply(self.preprocess_text)\n        return df\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-14T06:47:49.018645Z","iopub.execute_input":"2024-06-14T06:47:49.018954Z","iopub.status.idle":"2024-06-14T06:47:50.150357Z","shell.execute_reply.started":"2024-06-14T06:47:49.018929Z","shell.execute_reply":"2024-06-14T06:47:50.149435Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"# pipeline = Preprocessing()\n# df = pipeline.process_dataframe(df, \"problem\")","metadata":{"execution":{"iopub.status.busy":"2024-06-14T06:47:50.151508Z","iopub.execute_input":"2024-06-14T06:47:50.151779Z","iopub.status.idle":"2024-06-14T06:47:50.156656Z","shell.execute_reply.started":"2024-06-14T06:47:50.151756Z","shell.execute_reply":"2024-06-14T06:47:50.155850Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-06-14T06:47:50.157760Z","iopub.execute_input":"2024-06-14T06:47:50.158066Z","iopub.status.idle":"2024-06-14T06:47:50.178709Z","shell.execute_reply.started":"2024-06-14T06:47:50.158044Z","shell.execute_reply":"2024-06-14T06:47:50.177858Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                                                problem    level  \\\n0     The United States Postal Service charges an ex...  Level 3   \n1     How many integers between 1000 and 2000 have a...  Level 4   \n2     Given that $n$ is an integer and $0 < 4n <30$,...  Level 2   \n3     How many integers between $100$ and $150$ have...  Level 4   \n4     Regular pentagon $ABCDE$ and regular hexagon $...  Level 4   \n...                                                 ...      ...   \n7351  Find the number of real roots of\\n\\[2x^{2001} ...  Level 4   \n7352  For a complex number $z,$ compute the minimum ...  Level 4   \n7353  Compute the smallest positive integer $x$ grea...  Level 4   \n7354  For positive real numbers $a,$ $b,$ $c,$ and $...  Level 5   \n7355  Let $a,$ $b,$ and $c$ be positive real numbers...  Level 5   \n\n                      type                                           solution  \\\n0               Prealgebra  We calculate the desired ratio for each envelo...   \n1               Prealgebra  A number with 15, 20 and 25 as factors must be...   \n2               Prealgebra  Dividing by $4$, we have $0<n<7\\frac{1}{2}$. T...   \n3               Prealgebra  We will break up the problem into cases based ...   \n4               Prealgebra  We know that the sum of the degree measures of...   \n...                    ...                                                ...   \n7351  Intermediate Algebra  We can factor the given equation as\\n\\[(2x + 3...   \n7352  Intermediate Algebra  Geometrically, $|z + 5 - 3i|$ is the distance ...   \n7353  Intermediate Algebra  Let $q$ and $r$ be the remainder when $x$ is d...   \n7354  Intermediate Algebra  Let $S$ denote the given sum.  First, we apply...   \n7355  Intermediate Algebra  By AM-GM,\\n\\[(a - b) + b + \\frac{c^3}{(a - b)b...   \n\n     answer  is_integer  \n0         3        True  \n1         3        True  \n2        28        True  \n3        18        True  \n4       132        True  \n...     ...         ...  \n7351      1        True  \n7352     13        True  \n7353   1700        True  \n7354      9        True  \n7355     12        True  \n\n[7356 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>problem</th>\n      <th>level</th>\n      <th>type</th>\n      <th>solution</th>\n      <th>answer</th>\n      <th>is_integer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The United States Postal Service charges an ex...</td>\n      <td>Level 3</td>\n      <td>Prealgebra</td>\n      <td>We calculate the desired ratio for each envelo...</td>\n      <td>3</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>How many integers between 1000 and 2000 have a...</td>\n      <td>Level 4</td>\n      <td>Prealgebra</td>\n      <td>A number with 15, 20 and 25 as factors must be...</td>\n      <td>3</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Given that $n$ is an integer and $0 &lt; 4n &lt;30$,...</td>\n      <td>Level 2</td>\n      <td>Prealgebra</td>\n      <td>Dividing by $4$, we have $0&lt;n&lt;7\\frac{1}{2}$. T...</td>\n      <td>28</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>How many integers between $100$ and $150$ have...</td>\n      <td>Level 4</td>\n      <td>Prealgebra</td>\n      <td>We will break up the problem into cases based ...</td>\n      <td>18</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Regular pentagon $ABCDE$ and regular hexagon $...</td>\n      <td>Level 4</td>\n      <td>Prealgebra</td>\n      <td>We know that the sum of the degree measures of...</td>\n      <td>132</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7351</th>\n      <td>Find the number of real roots of\\n\\[2x^{2001} ...</td>\n      <td>Level 4</td>\n      <td>Intermediate Algebra</td>\n      <td>We can factor the given equation as\\n\\[(2x + 3...</td>\n      <td>1</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>7352</th>\n      <td>For a complex number $z,$ compute the minimum ...</td>\n      <td>Level 4</td>\n      <td>Intermediate Algebra</td>\n      <td>Geometrically, $|z + 5 - 3i|$ is the distance ...</td>\n      <td>13</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>7353</th>\n      <td>Compute the smallest positive integer $x$ grea...</td>\n      <td>Level 4</td>\n      <td>Intermediate Algebra</td>\n      <td>Let $q$ and $r$ be the remainder when $x$ is d...</td>\n      <td>1700</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>7354</th>\n      <td>For positive real numbers $a,$ $b,$ $c,$ and $...</td>\n      <td>Level 5</td>\n      <td>Intermediate Algebra</td>\n      <td>Let $S$ denote the given sum.  First, we apply...</td>\n      <td>9</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>7355</th>\n      <td>Let $a,$ $b,$ and $c$ be positive real numbers...</td>\n      <td>Level 5</td>\n      <td>Intermediate Algebra</td>\n      <td>By AM-GM,\\n\\[(a - b) + b + \\frac{c^3}{(a - b)b...</td>\n      <td>12</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n<p>7356 rows × 6 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import sympy as sp\nfrom sympy.parsing.latex import parse_latex\n!pip install antlr4-python3-runtime\ndef latex_to_math(latex_str):\n    \"\"\"\n    Convert a LaTeX string to a SymPy expression.\n\n    Parameters:\n    latex_str (str): The LaTeX string to convert.\n\n    Returns:\n    sympy.Expr: The corresponding SymPy expression.\n    \"\"\"\n    try:\n        # Parse the LaTeX string\n        sympy_expr = parse_latex(latex_str)\n        return sympy_expr\n    except Exception as e:\n        print(f\"Error parsing LaTeX: {e}\")\n        return None\n\n# Example usage\nlatex_str = r\"\\frac{d}{dx} \\left( x^2 + 2x + 1 \\right)\"\nmath_expr = latex_to_math(latex_str)\nprint(math_expr)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-14T06:47:50.182001Z","iopub.execute_input":"2024-06-14T06:47:50.182352Z","iopub.status.idle":"2024-06-14T06:48:03.345117Z","shell.execute_reply.started":"2024-06-14T06:47:50.182328Z","shell.execute_reply":"2024-06-14T06:48:03.343887Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting antlr4-python3-runtime\n  Downloading antlr4_python3_runtime-4.13.1-py3-none-any.whl.metadata (304 bytes)\nDownloading antlr4_python3_runtime-4.13.1-py3-none-any.whl (144 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: antlr4-python3-runtime\nSuccessfully installed antlr4-python3-runtime-4.13.1\nError parsing LaTeX: LaTeX parsing requires the antlr4 Python package, provided by pip (antlr4-python3-runtime) or conda (antlr-python-runtime), version 4.11\nNone\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Prompt Engineering","metadata":{}},{"cell_type":"code","source":"template = \"\"\"Role:\\nYou are an advanced AI system with exceptional mathematical reasoning and problem-solving capabilities, specifically designed to solve tricky math problems (whose answer is a non-negative integer) written in LaTeX format from the AI Mathematical Olympiad (AIMO) competition. Your task is to accurately analyze and solve intricate mathematical problems, demonstrating a deep understanding of mathematical concepts and a strong ability to apply logical reasoning strategies.\\n\\nInstruction:\n1. Carefully read and comprehend the problem statement provided in the \"Problem\" section.\n2. In the \"Solution\" section, provide a solution of the problem with detailed explanation of your logical reasoning process. Keep in mind that answer must be a non-negative integer number.\n3. At the end, create a \"Answer\" section where you will state only the final numerical or algebraic answer, without any additional text or narrative.\\n\\nProblem:\\n{problem}\\n\\nSolution:\\n{solution}\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-06-14T06:48:03.346576Z","iopub.execute_input":"2024-06-14T06:48:03.346955Z","iopub.status.idle":"2024-06-14T06:48:03.352225Z","shell.execute_reply.started":"2024-06-14T06:48:03.346923Z","shell.execute_reply":"2024-06-14T06:48:03.351226Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"df[\"prompt\"] = df.progress_apply(lambda row: template.format(problem=row.problem,\n                                                             solution=f\"{row.solution}\\n\\nAnswer:\\n{row.answer}\"),\n                                                             axis=1)\ndata = df.prompt.tolist()","metadata":{"execution":{"iopub.status.busy":"2024-06-14T06:48:03.353570Z","iopub.execute_input":"2024-06-14T06:48:03.353943Z","iopub.status.idle":"2024-06-14T06:48:03.700557Z","shell.execute_reply.started":"2024-06-14T06:48:03.353910Z","shell.execute_reply":"2024-06-14T06:48:03.699550Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/7356 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99231b1b0b0742ef89dd08b1c393fe1f"}},"metadata":{}}]},{"cell_type":"code","source":"def colorize_text(text):\n    for word, color in zip([\"Role\", \"Instruction\", \"Problem\", \"Solution\", \"Answer\"],\n                           [\"blue\", \"yellow\", \"red\", \"cyan\", \"green\"]):\n        text = text.replace(f\"{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n    return text","metadata":{"execution":{"iopub.status.busy":"2024-06-14T06:48:03.701998Z","iopub.execute_input":"2024-06-14T06:48:03.702639Z","iopub.status.idle":"2024-06-14T06:48:03.708573Z","shell.execute_reply.started":"2024-06-14T06:48:03.702604Z","shell.execute_reply":"2024-06-14T06:48:03.707684Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-06-14T06:48:03.722399Z","iopub.execute_input":"2024-06-14T06:48:03.722713Z","iopub.status.idle":"2024-06-14T06:48:03.741139Z","shell.execute_reply.started":"2024-06-14T06:48:03.722690Z","shell.execute_reply":"2024-06-14T06:48:03.740259Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 7356 entries, 0 to 7355\nData columns (total 7 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   problem     7356 non-null   object\n 1   level       7356 non-null   object\n 2   type        7356 non-null   object\n 3   solution    7356 non-null   object\n 4   answer      7356 non-null   object\n 5   is_integer  7356 non-null   bool  \n 6   prompt      7356 non-null   object\ndtypes: bool(1), object(6)\nmemory usage: 352.1+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"Markdown(colorize_text(df[\"prompt\"][0]))","metadata":{"execution":{"iopub.status.busy":"2024-06-14T06:48:03.742346Z","iopub.execute_input":"2024-06-14T06:48:03.742702Z","iopub.status.idle":"2024-06-14T06:48:03.753438Z","shell.execute_reply.started":"2024-06-14T06:48:03.742672Z","shell.execute_reply":"2024-06-14T06:48:03.752530Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"\n\n**<font color='blue'>Role:</font>**\nYou are an advanced AI system with exceptional mathematical reasoning and problem-solving capabilities, specifically designed to solve tricky math problems (whose answer is a non-negative integer) written in LaTeX format from the AI Mathematical Olympiad (AIMO) competition. Your task is to accurately analyze and solve intricate mathematical problems, demonstrating a deep understanding of mathematical concepts and a strong ability to apply logical reasoning strategies.\n\n\n\n**<font color='yellow'>Instruction:</font>**\n1. Carefully read and comprehend the problem statement provided in the \"Problem\" section.\n2. In the \"Solution\" section, provide a solution of the problem with detailed explanation of your logical reasoning process. Keep in mind that answer must be a non-negative integer number.\n3. At the end, create a \"Answer\" section where you will state only the final numerical or algebraic answer, without any additional text or narrative.\n\n\n\n**<font color='red'>Problem:</font>**\nThe United States Postal Service charges an extra $\\$0.11$ in postage if the length of an envelope, in inches, divided by its height, in inches, is less than $1.3$ or greater than $2.5.$ For how many of these four envelopes must the extra $\\$0.11$ in postage be paid? \\begin{tabular}[t]{ccc}\nEnvelope & Length in inches & Height in inches\\\\\\hline\nA &6 &4\\\\\nB &9 &3\\\\\nC &6 &6\\\\\nD &11 &4\n\\end{tabular}\n\n\n\n**<font color='cyan'>Solution:</font>**\nWe calculate the desired ratio for each envelope: \\begin{align*}\n\\text{A} &= \\frac{6}{4} = 1.5 \\\\\n\\text{B} &= \\frac{9}{3} = 3 \\\\\n\\text{C} &= \\frac{6}{6} = 1 \\\\\n\\text{D} &= \\frac{11}{4} = 2.75\n\\end{align*} $\\text B,$ $\\text C,$ and $\\text D$ are out of range, so the answer is $\\boxed{3}.$\n\n\n\n**<font color='green'>Answer:</font>**\n3"},"metadata":{}}]},{"cell_type":"code","source":"df[\"solution\"][0]","metadata":{"execution":{"iopub.status.busy":"2024-06-14T06:48:03.754622Z","iopub.execute_input":"2024-06-14T06:48:03.754954Z","iopub.status.idle":"2024-06-14T06:48:03.763614Z","shell.execute_reply.started":"2024-06-14T06:48:03.754929Z","shell.execute_reply":"2024-06-14T06:48:03.762837Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"'We calculate the desired ratio for each envelope: \\\\begin{align*}\\n\\\\text{A} &= \\\\frac{6}{4} = 1.5 \\\\\\\\\\n\\\\text{B} &= \\\\frac{9}{3} = 3 \\\\\\\\\\n\\\\text{C} &= \\\\frac{6}{6} = 1 \\\\\\\\\\n\\\\text{D} &= \\\\frac{11}{4} = 2.75\\n\\\\end{align*} $\\\\text B,$ $\\\\text C,$ and $\\\\text D$ are out of range, so the answer is $\\\\boxed{3}.$'"},"metadata":{}}]},{"cell_type":"markdown","source":"## Model Training","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\nimport transformers\nimport torch\n\nmodel = \"/kaggle/input/llama-3/transformers/8b-hf/1\"\n\ntokenizer = AutoTokenizer.from_pretrained(model, model_max_length=512)\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T06:48:03.764719Z","iopub.execute_input":"2024-06-14T06:48:03.765058Z","iopub.status.idle":"2024-06-14T06:50:14.507366Z","shell.execute_reply.started":"2024-06-14T06:48:03.765029Z","shell.execute_reply":"2024-06-14T06:50:14.506585Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"772867c4df394501960fa781f6202206"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"def split_text_into_prompt_completion(df, text_column):\n    prompts = []\n    completions = []\n\n    for index, row in df.iterrows():\n        text = row[text_column]\n        \n        # Split based on \"Solution:\"\n        problem_part, solution_part = text.split(\"Solution:\", 1)\n        \n        # Ensure to keep the \"Solution:\" keyword in the completion\n        solution_part = \"Solution:\" + solution_part\n        \n        # Append to lists\n        prompts.append(problem_part.strip())\n        completions.append(solution_part.strip())\n    \n    # Create new DataFrame\n    split_df = pd.DataFrame({\n        \"prompt\": prompts,\n        \"completion\": completions\n    })\n    \n    return split_df\n\n# Example usage\ndata = split_text_into_prompt_completion(df, 'prompt')","metadata":{"execution":{"iopub.status.busy":"2024-06-14T06:50:14.508703Z","iopub.execute_input":"2024-06-14T06:50:14.509353Z","iopub.status.idle":"2024-06-14T06:50:14.918847Z","shell.execute_reply.started":"2024-06-14T06:50:14.509318Z","shell.execute_reply":"2024-06-14T06:50:14.918071Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2024-06-14T06:50:14.919900Z","iopub.execute_input":"2024-06-14T06:50:14.920184Z","iopub.status.idle":"2024-06-14T06:50:14.930885Z","shell.execute_reply.started":"2024-06-14T06:50:14.920160Z","shell.execute_reply":"2024-06-14T06:50:14.929962Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"                                                 prompt  \\\n0     Role:\\nYou are an advanced AI system with exce...   \n1     Role:\\nYou are an advanced AI system with exce...   \n2     Role:\\nYou are an advanced AI system with exce...   \n3     Role:\\nYou are an advanced AI system with exce...   \n4     Role:\\nYou are an advanced AI system with exce...   \n...                                                 ...   \n7351  Role:\\nYou are an advanced AI system with exce...   \n7352  Role:\\nYou are an advanced AI system with exce...   \n7353  Role:\\nYou are an advanced AI system with exce...   \n7354  Role:\\nYou are an advanced AI system with exce...   \n7355  Role:\\nYou are an advanced AI system with exce...   \n\n                                             completion  \n0     Solution:\\nWe calculate the desired ratio for ...  \n1     Solution:\\nA number with 15, 20 and 25 as fact...  \n2     Solution:\\nDividing by $4$, we have $0<n<7\\fra...  \n3     Solution:\\nWe will break up the problem into c...  \n4     Solution:\\nWe know that the sum of the degree ...  \n...                                                 ...  \n7351  Solution:\\nWe can factor the given equation as...  \n7352  Solution:\\nGeometrically, $|z + 5 - 3i|$ is th...  \n7353  Solution:\\nLet $q$ and $r$ be the remainder wh...  \n7354  Solution:\\nLet $S$ denote the given sum.  Firs...  \n7355  Solution:\\nBy AM-GM,\\n\\[(a - b) + b + \\frac{c^...  \n\n[7356 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt</th>\n      <th>completion</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Role:\\nYou are an advanced AI system with exce...</td>\n      <td>Solution:\\nWe calculate the desired ratio for ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Role:\\nYou are an advanced AI system with exce...</td>\n      <td>Solution:\\nA number with 15, 20 and 25 as fact...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Role:\\nYou are an advanced AI system with exce...</td>\n      <td>Solution:\\nDividing by $4$, we have $0&lt;n&lt;7\\fra...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Role:\\nYou are an advanced AI system with exce...</td>\n      <td>Solution:\\nWe will break up the problem into c...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Role:\\nYou are an advanced AI system with exce...</td>\n      <td>Solution:\\nWe know that the sum of the degree ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7351</th>\n      <td>Role:\\nYou are an advanced AI system with exce...</td>\n      <td>Solution:\\nWe can factor the given equation as...</td>\n    </tr>\n    <tr>\n      <th>7352</th>\n      <td>Role:\\nYou are an advanced AI system with exce...</td>\n      <td>Solution:\\nGeometrically, $|z + 5 - 3i|$ is th...</td>\n    </tr>\n    <tr>\n      <th>7353</th>\n      <td>Role:\\nYou are an advanced AI system with exce...</td>\n      <td>Solution:\\nLet $q$ and $r$ be the remainder wh...</td>\n    </tr>\n    <tr>\n      <th>7354</th>\n      <td>Role:\\nYou are an advanced AI system with exce...</td>\n      <td>Solution:\\nLet $S$ denote the given sum.  Firs...</td>\n    </tr>\n    <tr>\n      <th>7355</th>\n      <td>Role:\\nYou are an advanced AI system with exce...</td>\n      <td>Solution:\\nBy AM-GM,\\n\\[(a - b) + b + \\frac{c^...</td>\n    </tr>\n  </tbody>\n</table>\n<p>7356 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"tokenized_data = []\nprompt = data[\"prompt\"]\ncompletion = data[\"completion\"]\ninput_texts = [prompt + tokenizer.eos_token + completion for prompt, completion in zip(data[\"prompt\"], data[\"completion\"])]\n\nfor input_text in input_texts:\n    tokenized_input = tokenizer(input_text, return_tensors='pt', padding=True, truncation=True)\n    tokenized_data.append(tokenized_input)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T06:53:14.084959Z","iopub.execute_input":"2024-06-14T06:53:14.085346Z","iopub.status.idle":"2024-06-14T06:53:23.450460Z","shell.execute_reply.started":"2024-06-14T06:53:14.085317Z","shell.execute_reply":"2024-06-14T06:53:23.449635Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\n\n# Create a list of dictionaries with 'input_ids' and 'attention_mask'\ntrain_data = {\n    'input_ids': [tokenized_input['input_ids'].squeeze().tolist() for tokenized_input in tokenized_data],\n    'attention_mask': [tokenized_input['attention_mask'].squeeze().tolist() for tokenized_input in tokenized_data]\n}\n\ntrain_dataset = Dataset.from_dict(train_data)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T07:09:05.418111Z","iopub.execute_input":"2024-06-14T07:09:05.418904Z","iopub.status.idle":"2024-06-14T07:09:06.880999Z","shell.execute_reply.started":"2024-06-14T07:09:05.418873Z","shell.execute_reply":"2024-06-14T07:09:06.880225Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, Trainer, TrainingArguments\n\nmodel = AutoModelForCausalLM.from_pretrained(model, torch_dtype=torch.float16)\n\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,\n    per_device_train_batch_size=1,\n    save_steps=10_000,\n    save_total_limit=2,\n    logging_dir='./logs',\n    logging_steps=200,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    tokenizer=tokenizer,\n)\n\n# Train the model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-06-14T07:09:10.559293Z","iopub.execute_input":"2024-06-14T07:09:10.560030Z","iopub.status.idle":"2024-06-14T07:10:15.583370Z","shell.execute_reply.started":"2024-06-14T07:09:10.559999Z","shell.execute_reply":"2024-06-14T07:10:15.580861Z"},"trusted":true},"execution_count":31,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efd7f0b5813e49f4bfeaf89e0634ac24"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[31], line 15\u001b[0m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model, torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16)\n\u001b[1;32m      5\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      6\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     logging_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 15\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m     23\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:528\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;66;03m# Bnb Quantized models doesn't support `.to` operation.\u001b[39;00m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplace_model_on_device\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mBITS_AND_BYTES\n\u001b[1;32m    527\u001b[0m ):\n\u001b[0;32m--> 528\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_move_model_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;66;03m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[39;00m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_model_parallel:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:775\u001b[0m, in \u001b[0;36mTrainer._move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    774\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_move_model_to_device\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, device):\n\u001b[0;32m--> 775\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    776\u001b[0m     \u001b[38;5;66;03m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[39;00m\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mparallel_mode \u001b[38;5;241m==\u001b[39m ParallelMode\u001b[38;5;241m.\u001b[39mTPU \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtie_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:2724\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2719\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2720\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2721\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2722\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2723\u001b[0m         )\n\u001b[0;32m-> 2724\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","    \u001b[0;31m[... skipping similar frames: Module._apply at line 810 (2 times)]\u001b[0m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 70.12 MiB is free. Process 2294 has 15.82 GiB memory in use. Of the allocated memory 15.44 GiB is allocated by PyTorch, and 137.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 70.12 MiB is free. Process 2294 has 15.82 GiB memory in use. Of the allocated memory 15.44 GiB is allocated by PyTorch, and 137.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error"}]},{"cell_type":"code","source":"trainer.save_model(\"path_to_save_your_model\")\ntokenizer.save_pretrained(\"path_to_save_your_model\")","metadata":{"execution":{"iopub.status.busy":"2024-06-14T06:50:16.155687Z","iopub.status.idle":"2024-06-14T06:50:16.156162Z","shell.execute_reply.started":"2024-06-14T06:50:16.155924Z","shell.execute_reply":"2024-06-14T06:50:16.155943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline\n\n# Load the fine-tuned model and tokenizer\nfine_tuned_model = AutoModelForCausalLM.from_pretrained(\"path_to_save_your_model\")\nfine_tuned_tokenizer = AutoTokenizer.from_pretrained(\"path_to_save_your_model\")\n\n# Create a pipeline\ngeneration_pipeline = pipeline(\n    \"text-generation\",\n    model=fine_tuned_model,\n    tokenizer=fine_tuned_tokenizer,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\n\n# Generate text\nprompt = \"Your test prompt\"\ngenerated_text = generation_pipeline(prompt)\nprint(generated_text)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T06:50:16.157416Z","iopub.status.idle":"2024-06-14T06:50:16.157912Z","shell.execute_reply.started":"2024-06-14T06:50:16.157627Z","shell.execute_reply":"2024-06-14T06:50:16.157646Z"},"trusted":true},"execution_count":null,"outputs":[]}]}